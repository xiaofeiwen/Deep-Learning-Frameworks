{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "%matplotlib inline\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tf_data():\n",
    "    mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "    X_train = mnist.train.images\n",
    "    Y_train = mnist.train.labels\n",
    "    X_test = mnist.test.images\n",
    "    Y_test = mnist.test.labels\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_kaggle_data():\n",
    "    data = pd.read_csv('input/train.csv')\n",
    "    X = data.iloc[:,1:].values\n",
    "    X = X.astype(np.float)\n",
    "    Y = data.iloc[:,0].values\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=2000, random_state=42)\n",
    "    X_train = X_train.reshape(-1,28,28,1)/255.\n",
    "    X_test = X_test.reshape(-1,28,28,1)/255.\n",
    "    Y_train = np.eye(10)[Y_train.reshape(-1)]\n",
    "    Y_test = np.eye(10)[Y_test.reshape(-1)]\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_tf_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model1(X_train, Y_train, X_test, Y_test,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    t=0\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    m = X_train.shape[0]                          # (n_x: input size, m : number of examples in the train set)\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    # correct answers will go here\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "    L = 200\n",
    "    M = 100\n",
    "    N = 60\n",
    "    O = 30\n",
    "    # Weights initialised with small random values between -0.2 and +0.2\n",
    "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "    B1 = tf.Variable(tf.ones([L])/10)\n",
    "    W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.ones([M])/10)\n",
    "    W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.ones([N])/10)\n",
    "    W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.ones([O])/10)\n",
    "    W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"B1\": B1,\n",
    "                  \"W2\": W2,\n",
    "                  \"B2\": B2,\n",
    "                  \"W3\": W3,\n",
    "                  \"B3\": B3,\n",
    "                  \"W4\": W4,\n",
    "                  \"B4\": B4,\n",
    "                  \"W5\": W5,\n",
    "                  \"B5\": B5}\n",
    "\n",
    "    # The model, with dropout at each layer\n",
    "    XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "    Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\n",
    "    Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "    Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "    Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "    Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "    Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "    Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "    Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "    Ylogits = tf.matmul(Y4d, W5) + B5\n",
    "#     Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = Y))\n",
    "\n",
    "    # training step, the learning rate is a placeholder\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # set learning rate decay\n",
    "                max_learning_rate = 0.003\n",
    "                min_learning_rate = 0.0001\n",
    "                decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-t/decay_speed)\n",
    "                t += 1\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y,\n",
    "                                                                            pkeep: 0.75, lr: learning_rate})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, pkeep: 1}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, pkeep: 1}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "def model2(X_train, Y_train, X_test, Y_test,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    t=0\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                          \n",
    "    m = X_train.shape[0]                          \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    # correct answers will go here\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    itr = tf.placeholder(tf.int32)\n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "    L = 200\n",
    "    M = 100\n",
    "    N = 60\n",
    "    O = 30\n",
    "    # Weights initialised with small random values between -0.2 and +0.2\n",
    "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "    B1 = tf.Variable(tf.ones([L])/10)\n",
    "    W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.ones([M])/10)\n",
    "    W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.ones([N])/10)\n",
    "    W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.ones([O])/10)\n",
    "    W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"B1\": B1,\n",
    "                  \"W2\": W2,\n",
    "                  \"B2\": B2,\n",
    "                  \"W3\": W3,\n",
    "                  \"B3\": B3,\n",
    "                  \"W4\": W4,\n",
    "                  \"B4\": B4,\n",
    "                  \"W5\": W5,\n",
    "                  \"B5\": B5}\n",
    "\n",
    "    # The model\n",
    "    XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "    # batch norm scaling is not useful with relus\n",
    "    # batch norm offsets are used instead of biases\n",
    "\n",
    "    Y1l = tf.matmul(XX, W1)\n",
    "    Y1bn, update_ema1 = batchnorm(Y1l, tst, itr, B1)\n",
    "    Y1 = tf.nn.relu(Y1bn)\n",
    "    Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "    Y2l = tf.matmul(Y1d, W2)\n",
    "    Y2bn, update_ema2 = batchnorm(Y2l, tst, itr, B2)\n",
    "    Y2 = tf.nn.relu(Y2bn)\n",
    "    Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "    Y3l = tf.matmul(Y2d, W3)\n",
    "    Y3bn, update_ema3 = batchnorm(Y3l, tst, itr, B3)\n",
    "    Y3 = tf.nn.relu(Y3bn)\n",
    "    Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "    Y4l = tf.matmul(Y3d, W4)\n",
    "    Y4bn, update_ema4 = batchnorm(Y4l, tst, itr, B4)\n",
    "    Y4 = tf.nn.relu(Y4bn)\n",
    "    Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "    Ylogits = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "    update_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = Y))\n",
    "\n",
    "    # training step, the learning rate is a placeholder\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # set learning rate decay\n",
    "                max_learning_rate = 0.003\n",
    "                min_learning_rate = 0.0001\n",
    "                decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-t/decay_speed)\n",
    "                t += 1\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer,cost], {X:minibatch_X, Y:minibatch_Y, pkeep:1,\n",
    "                                                                 lr: learning_rate, tst: False})\n",
    "                sess.run(update_ema, {X:minibatch_X, Y:minibatch_Y, tst: False, itr: t, pkeep:1})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, pkeep: 1,tst: False}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, pkeep: 1,tst: False}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model3(X_train, Y_train, X_test, Y_test,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    t=0\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    m = X_train.shape[0]                          # (n_x: input size, m : number of examples in the train set)\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    # correct answers will go here\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    # three convolutional layers with their channel counts, and a\n",
    "    # fully connected layer (the last layer has 10 softmax neurons)\n",
    "    K = 6  # first convolutional layer output depth\n",
    "    L = 12  # second convolutional layer output depth\n",
    "    M = 24  # third convolutional layer\n",
    "    N = 200  # fully connected layer\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "\n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"B1\": B1,\n",
    "                  \"W2\": W2,\n",
    "                  \"B2\": B2,\n",
    "                  \"W3\": W3,\n",
    "                  \"B3\": B3,\n",
    "                  \"W4\": W4,\n",
    "                  \"B4\": B4,\n",
    "                  \"W5\": W5,\n",
    "                  \"B5\": B5}\n",
    "\n",
    "    # The model\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    Ylogits = tf.matmul(YY4, W5) + B5\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = Y))\n",
    "\n",
    "    # training step, the learning rate is a placeholder\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # set learning rate decay\n",
    "                max_learning_rate = 0.003\n",
    "                min_learning_rate = 0.0001\n",
    "                decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-t/decay_speed)\n",
    "                t += 1\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y,\n",
    "                                                                            pkeep: 0.75, lr: learning_rate})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, pkeep: 1}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, pkeep: 1}))\n",
    "        \n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    return Ylogits, tf.no_op()\n",
    "\n",
    "def compatible_convolutional_noise_shape(Y):\n",
    "    noiseshape = tf.shape(Y)\n",
    "    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
    "    return noiseshape\n",
    "\n",
    "def model4(X_train, Y_train, X_test, Y_test,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    t=0\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                          \n",
    "    m = X_train.shape[0]                          \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    # correct answers will go here\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    itr = tf.placeholder(tf.int32)\n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    pkeep_conv = tf.placeholder(tf.float32)\n",
    "\n",
    "    # three convolutional layers with their channel counts, and a\n",
    "    # fully connected layer (tha last layer has 10 softmax neurons)\n",
    "    K = 24  # first convolutional layer output depth\n",
    "    L = 48  # second convolutional layer output depth\n",
    "    M = 64  # third convolutional layer\n",
    "    N = 200  # fully connected layer\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "\n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"B1\": B1,\n",
    "                  \"W2\": W2,\n",
    "                  \"B2\": B2,\n",
    "                  \"W3\": W3,\n",
    "                  \"B3\": B3,\n",
    "                  \"W4\": W4,\n",
    "                  \"B4\": B4,\n",
    "                  \"W5\": W5,\n",
    "                  \"B5\": B5}\n",
    "\n",
    "    # The model\n",
    "    # batch norm scaling is not useful with relus\n",
    "    # batch norm offsets are used instead of biases\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1l = tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    Y1bn, update_ema1 = batchnorm(Y1l, tst, itr, B1, convolutional=True)\n",
    "    Y1r = tf.nn.relu(Y1bn)\n",
    "    Y1 = tf.nn.dropout(Y1r, pkeep_conv, compatible_convolutional_noise_shape(Y1r))\n",
    "    stride = 2  # output is 14x14\n",
    "    Y2l = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    Y2bn, update_ema2 = batchnorm(Y2l, tst, itr, B2, convolutional=True)\n",
    "    Y2r = tf.nn.relu(Y2bn)\n",
    "    Y2 = tf.nn.dropout(Y2r, pkeep_conv, compatible_convolutional_noise_shape(Y2r))\n",
    "    stride = 2  # output is 7x7\n",
    "    Y3l = tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    Y3bn, update_ema3 = batchnorm(Y3l, tst, itr, B3, convolutional=True)\n",
    "    Y3r = tf.nn.relu(Y3bn)\n",
    "    Y3 = tf.nn.dropout(Y3r, pkeep_conv, compatible_convolutional_noise_shape(Y3r))\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "    Y4l = tf.matmul(YY, W4)\n",
    "    Y4bn, update_ema4 = batchnorm(Y4l, tst, itr, B4)\n",
    "    Y4r = tf.nn.relu(Y4bn)\n",
    "    Y4 = tf.nn.dropout(Y4r, pkeep)\n",
    "    Ylogits = tf.matmul(Y4, W5) + B5\n",
    "\n",
    "    update_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = Y))\n",
    "\n",
    "    # training step, the learning rate is a placeholder\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # set learning rate decay\n",
    "                max_learning_rate = 0.003\n",
    "                min_learning_rate = 0.0001\n",
    "                decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-t/decay_speed)\n",
    "                t += 1\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer,cost], {X:minibatch_X, Y:minibatch_Y, pkeep:0.75, pkeep_conv:1,\n",
    "                                                                 lr: learning_rate, tst: False})\n",
    "                sess.run(update_ema, {X:minibatch_X, Y:minibatch_Y, tst: False, itr: t, pkeep:1,pkeep_conv:1})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                break\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Ylogits, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, pkeep: 1,pkeep_conv:1,tst: False}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, pkeep: 1,pkeep_conv:1,tst: False}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.225864\n",
      "Cost after epoch 1: 0.064315\n",
      "Cost after epoch 2: 0.040767\n",
      "Cost after epoch 3: 0.030529\n",
      "Cost after epoch 4: 0.020573\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKLAEBGRSBQYIiISiQJDREQi+T8iNQOhnWwUwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc91204b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.997699\n",
      "Test Accuracy: 0.988\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = load_tf_data()\n",
    "X_train, X_test, Y_train, Y_test = load_kaggle_data()\n",
    "parameters = model3(X_train, Y_train, X_test, Y_test, num_epochs = 5, minibatch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict3(X_to_predict, parameters):\n",
    "    \n",
    "    M = 24  # third convolutional layer\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    B1 = tf.convert_to_tensor(parameters[\"B1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    B2 = tf.convert_to_tensor(parameters[\"B2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    B3 = tf.convert_to_tensor(parameters[\"B3\"])\n",
    "    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n",
    "    B4 = tf.convert_to_tensor(parameters[\"B4\"])\n",
    "    W5 = tf.convert_to_tensor(parameters[\"W5\"])\n",
    "    B5 = tf.convert_to_tensor(parameters[\"B5\"])\n",
    "    \n",
    "#     params = {\"W1\": W1,\n",
    "#               \"b1\": b1,\n",
    "#               \"W2\": W2,\n",
    "#               \"b2\": b2,\n",
    "#               \"W3\": W3,\n",
    "#               \"b3\": b3}\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    \n",
    "    # The model\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "#     YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    Ylogits = tf.matmul(Y4, W5) + B5\n",
    "    \n",
    "    p = tf.argmax(Ylogits,1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        prediction = sess.run(p, feed_dict = {X: X_to_predict})\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_kaggle_test():\n",
    "    # read test data from CSV file \n",
    "    test_images = pd.read_csv('input/test.csv').values\n",
    "    test_images = test_images.astype(np.float)\n",
    "\n",
    "    # convert from [0:255] => [0.0:1.0]\n",
    "    test_images = test_images.reshape(-1,28,28,1)/255.\n",
    "    return test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = load_kaggle_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = predict3(test_images, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1391de80>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1376e828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('submission_model3.csv', \n",
    "           np.c_[range(1,len(test_images)+1),prediction], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
